{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4-  Feature Space Visualisation\n",
    "Author : @leopauly | cnlp@leeds.ac.uk <br>\n",
    "Description : Studying the feature vectors in the feature space by visualising them using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import PIL.Image as Image\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from skimage.color import grey2rgb,rgb2grey\n",
    "from skimage.feature import hog\n",
    "from skimage import io\n",
    "\n",
    "# Custom scripts\n",
    "import lscript as lsp\n",
    "import modelling as md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "height=112 \n",
    "width=112 \n",
    "channel=3\n",
    "crop_size=112\n",
    "cluster_length=16\n",
    "feature_size=8192 #4096 #16384\n",
    "baseline_feature_size=4608\n",
    "baseline2_feature_size=2\n",
    "\n",
    "\n",
    "nb_activities=2\n",
    "nb_videos=8\n",
    "nb_pixels_per_cell=(8,8)\n",
    "nb_cells_per_block=(1,1)\n",
    "nb_orientations=16\n",
    "saved_path='/nobackup/leopauly/logdirk80_1_rand_frames'\n",
    "dataset_dir='/nobackup/leopauly/IROSDataset/VizData/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acitivity net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/home01/cnlp/Seeing_to_Learn/Observation-Learning/ExperimentSet_ECCV/modelling.py:281: UserWarning: Update your `Conv3D` call to the Keras 2 API: `Conv3D(64, (3, 3, 3), activation=\"relu\", name=\"conv1\", input_shape=(16, 112, ..., padding=\"same\")`\n",
      "  input_shape=input_shape))\n",
      "/home/home01/cnlp/Seeing_to_Learn/Observation-Learning/ExperimentSet_ECCV/modelling.py:283: UserWarning: Update your `MaxPooling3D` call to the Keras 2 API: `MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name=\"pool1\", padding=\"valid\")`\n",
      "  border_mode='valid', name='pool1'))\n",
      "/home/home01/cnlp/Seeing_to_Learn/Observation-Learning/ExperimentSet_ECCV/modelling.py:286: UserWarning: Update your `Conv3D` call to the Keras 2 API: `Conv3D(128, (3, 3, 3), activation=\"relu\", name=\"conv2\", padding=\"same\")`\n",
      "  border_mode='same', name='conv2'))\n",
      "/home/home01/cnlp/Seeing_to_Learn/Observation-Learning/ExperimentSet_ECCV/modelling.py:288: UserWarning: Update your `MaxPooling3D` call to the Keras 2 API: `MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), name=\"pool2\", padding=\"valid\")`\n",
      "  border_mode='valid', name='pool2'))\n",
      "/home/home01/cnlp/Seeing_to_Learn/Observation-Learning/ExperimentSet_ECCV/modelling.py:291: UserWarning: Update your `Conv3D` call to the Keras 2 API: `Conv3D(256, (3, 3, 3), activation=\"relu\", name=\"conv3a\", padding=\"same\")`\n",
      "  border_mode='same', name='conv3a'))\n",
      "/home/home01/cnlp/Seeing_to_Learn/Observation-Learning/ExperimentSet_ECCV/modelling.py:293: UserWarning: Update your `Conv3D` call to the Keras 2 API: `Conv3D(256, (3, 3, 3), activation=\"relu\", name=\"conv3b\", padding=\"same\")`\n",
      "  border_mode='same', name='conv3b'))\n",
      "/home/home01/cnlp/Seeing_to_Learn/Observation-Learning/ExperimentSet_ECCV/modelling.py:295: UserWarning: Update your `MaxPooling3D` call to the Keras 2 API: `MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), name=\"pool3\", padding=\"valid\")`\n",
      "  border_mode='valid', name='pool3'))\n",
      "/home/home01/cnlp/Seeing_to_Learn/Observation-Learning/ExperimentSet_ECCV/modelling.py:298: UserWarning: Update your `Conv3D` call to the Keras 2 API: `Conv3D(512, (3, 3, 3), activation=\"relu\", name=\"conv4a\", padding=\"same\")`\n",
      "  border_mode='same', name='conv4a'))\n",
      "/home/home01/cnlp/Seeing_to_Learn/Observation-Learning/ExperimentSet_ECCV/modelling.py:300: UserWarning: Update your `Conv3D` call to the Keras 2 API: `Conv3D(512, (3, 3, 3), activation=\"relu\", name=\"conv4b\", padding=\"same\")`\n",
      "  border_mode='same', name='conv4b'))\n",
      "/home/home01/cnlp/Seeing_to_Learn/Observation-Learning/ExperimentSet_ECCV/modelling.py:302: UserWarning: Update your `MaxPooling3D` call to the Keras 2 API: `MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), name=\"pool4\", padding=\"valid\")`\n",
      "  border_mode='valid', name='pool4'))\n",
      "/home/home01/cnlp/Seeing_to_Learn/Observation-Learning/ExperimentSet_ECCV/modelling.py:305: UserWarning: Update your `Conv3D` call to the Keras 2 API: `Conv3D(512, (3, 3, 3), activation=\"relu\", name=\"conv5a\", padding=\"same\")`\n",
      "  border_mode='same', name='conv5a'))\n",
      "/home/home01/cnlp/Seeing_to_Learn/Observation-Learning/ExperimentSet_ECCV/modelling.py:307: UserWarning: Update your `Conv3D` call to the Keras 2 API: `Conv3D(512, (3, 3, 3), activation=\"relu\", name=\"conv5b\", padding=\"same\")`\n",
      "  border_mode='same', name='conv5b'))\n",
      "/home/home01/cnlp/Seeing_to_Learn/Observation-Learning/ExperimentSet_ECCV/modelling.py:310: UserWarning: Update your `MaxPooling3D` call to the Keras 2 API: `MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), name=\"pool5\", padding=\"valid\")`\n",
      "  border_mode='valid', name='pool5'))\n"
     ]
    }
   ],
   "source": [
    "## Defining placeholders in tf for images and targets\n",
    "x_image = tf.placeholder(tf.float32, [None, 16,height,width,channel],name='x') \n",
    "\n",
    "model_keras = md.C3D_ucf101_training_model_tf(summary=False)\n",
    "out=model_keras(x_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting a session - Activity Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /nobackup/leopauly/logdirk80_1_rand_frames/activity_model.ckpt-104\n",
      "Model restored from file: /nobackup/leopauly/logdirk80_1_rand_frames\n"
     ]
    }
   ],
   "source": [
    "#### Start the session with logging placement.\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False))\n",
    "\n",
    "### Restore model weights from previously saved model\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, os.path.join(saved_path,'activity_model.ckpt-104'))\n",
    "print(\"Model restored from file: %s\" % saved_path,flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Extraction of features - baseline\n",
    "def extract_activitynet_video_features(vid):\n",
    "    print(vid.shape)\n",
    "    vid_=vid.reshape(-1,cluster_length,height,width,channel)\n",
    "    f_v = sess.graph.get_tensor_by_name('flatten_1/Reshape:0')\n",
    "    f_v_val=sess.run([f_v], feed_dict={'conv1_input:0':vid_,x_image:vid_,K.learning_phase(): 0 })\n",
    "    features=np.reshape(f_v_val,(-1))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Org\n",
    "def get_compress_frames_data(filename, num_frames_per_clip=cluster_length):\n",
    "  ''' Given a directory containing extracted frames, return a video clip of\n",
    "  (num_frames_per_clip) consecutive frames as a list of np arrays '''\n",
    "  \n",
    "  ret_arr = []\n",
    "  for parent, dirnames, filenames in os.walk(filename):\n",
    "\n",
    "    filenames = sorted(filenames)\n",
    "    #print(filenames)\n",
    "    jump=math.floor((len(filenames)/num_frames_per_clip))\n",
    "    loop=0\n",
    "    for i in range(0,len(filenames),jump):\n",
    "      if (loop>15):\n",
    "        break\n",
    "      if (filenames[i].endswith('.png')):\n",
    "        image_name = str(filename) + '/' + str(filenames[i])\n",
    "        img = Image.open(image_name)\n",
    "        img_data = np.array(img)\n",
    "        ret_arr.append(img_data)\n",
    "        loop=loop+1\n",
    "  ret_arr=np.array(ret_arr)\n",
    "  #ret_arr=ret_arr/255\n",
    "  return np.array(ret_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Dup\n",
    "def get_compress_frames_data(filename, num_frames_per_clip=cluster_length):\n",
    "  ''' Given a directory containing extracted frames, return a video clip of\n",
    "  (num_frames_per_clip) consecutive frames as a list of np arrays '''\n",
    "  ret_arr = []\n",
    "  s_index = 0\n",
    "  for parent, dirnames, filenames in os.walk(filename):\n",
    "    if(len(filenames)<num_frames_per_clip):\n",
    "      return [], s_index\n",
    "    filenames = sorted(filenames)\n",
    "    s_index = random.randint(0, len(filenames) - num_frames_per_clip)\n",
    "    for i in range(s_index, s_index + num_frames_per_clip):\n",
    "      image_name = str(filename) + '/' + str(filenames[i])\n",
    "      img = Image.open(image_name)\n",
    "      img_data = np.array(img)\n",
    "      #lsp.view_image(img_data)\n",
    "      ret_arr.append(img_data)\n",
    "  ret_arr=np.array(ret_arr)\n",
    "  #ret_arr=ret_arr/255\n",
    "  print('ret_arr',np.array(ret_arr).shape)\n",
    "  return ret_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 8)\n",
      "activity_folders: ['pour', 'push']\n",
      "['0deg', '0degC', '180deg', '180degC', '270deg', '270degC', '90deg', '90degC']\n",
      "activity subfolders in pour folder : ['0deg', '0degC', '180deg', '180degC', '270deg', '270degC', '90deg', '90degC']\n",
      "['0deg', '0degC', '180deg', '180degC', '270deg', '270degC', '90deg', '90degC']\n",
      "activity subfolders in push folder : ['0deg', '0degC', '180deg', '180degC', '270deg', '270degC', '90deg', '90degC']\n"
     ]
    }
   ],
   "source": [
    "activity_folders=sorted(os.listdir(dataset_dir))\n",
    "activity_subfolders= [['foo' for i in range(nb_videos)] for j in range(nb_activities)]\n",
    "k=np.array(activity_subfolders)\n",
    "print(k.shape)\n",
    "print('activity_folders:',activity_folders)\n",
    "for i,folder in enumerate(activity_folders):\n",
    "    activity_subfolders[i]=sorted(os.listdir(dataset_dir+'/'+folder))\n",
    "    print(sorted(os.listdir(dataset_dir+'/'+folder)))\n",
    "    print('activity subfolders in',folder,'folder :', activity_subfolders[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ret_arr (16, 112, 112, 3)\n",
      "/nobackup/leopauly/IROSDataset/VizData/pour/0deg\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d907efdad0b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_compress_frames_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivity_folders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnb_act\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivity_subfolders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnb_act\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnb_vid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivity_folders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnb_act\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivity_subfolders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnb_act\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnb_vid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mactivitynet_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnb_act\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnb_vid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextract_activitynet_video_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_act\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mptr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-c79d7bd12c8e>\u001b[0m in \u001b[0;36mextract_activitynet_video_features\u001b[0;34m(vid)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Extraction of features - baseline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_activitynet_video_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mvid_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcluster_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchannel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mf_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'flatten_1/Reshape:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "## Extracting activity features from each class videos\n",
    "activitynet_features=[[[0 for x in range(feature_size)] for y in range(nb_videos)] for z in range(nb_activities)]\n",
    "labels=np.zeros([nb_activities*nb_videos])\n",
    "\n",
    "ptr=0\n",
    "for nb_act in range(nb_activities):\n",
    "    for nb_vid in range(nb_videos):\n",
    "        temp=0\n",
    "        temp=get_compress_frames_data(dataset_dir+str(activity_folders[nb_act])+'/'+str(activity_subfolders[nb_act][nb_vid]))\n",
    "        print(dataset_dir+str(activity_folders[nb_act])+'/'+str(activity_subfolders[nb_act][nb_vid]))\n",
    "        activitynet_features[nb_act][nb_vid]=extract_activitynet_video_features(temp)\n",
    "        labels[ptr]=nb_act\n",
    "        ptr=ptr+1\n",
    "activitynet_features=np.array(activitynet_features)\n",
    "ptr=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_obj = PCA(n_components=2, random_state=0)\n",
    "vis_activitynet=[pca_obj.fit_transform(activitynet_features[i]) for i  in range(nb_activities)]\n",
    "vis_activitynet=np.array(vis_activitynet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_obj = TSNE(n_components=2, verbose=1)\n",
    "vis_activitynet_tsne=[tsne_obj.fit_transform(activitynet_features[i]) for i  in range(nb_activities)]\n",
    "vis_activitynet_tsne=np.array(vis_activitynet_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close() \n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model=md.get_vgg16_imagenet(summary=True,include_fc=False)\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Extraction of features - baseline\n",
    "def extract_baseline_video_features(vid):\n",
    "    \n",
    "    sum_val=0\n",
    "    for i in range(cluster_length):\n",
    "        frame_=vid[i]\n",
    "        \n",
    "        #frame_=frame_.reshape(-1,height,width,channel)\n",
    "        #frame= preprocess_input(frame_) # prepare the image for the VGG model\n",
    "        \n",
    "        frame=preprocess(frame_)\n",
    "        frame=frame.reshape(-1,height,width,channel)\n",
    "        \n",
    "        temp_val=base_model.predict(frame)\n",
    "        temp_val=temp_val.reshape(baseline_feature_size)\n",
    "        #print('temp_val',temp_val.shape)\n",
    "        sum_val=sum_val+temp_val\n",
    "        #print('sum_val',sum_val.shape)\n",
    "    features=sum_val/cluster_length\n",
    "    #print('feature from one video.shape',features.shape)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(im):\n",
    "        im = np.float32(im)\n",
    "        im[:,:,2] -= 103.939\n",
    "        im[:,:,1] -= 116.779\n",
    "        im[:,:,0] -= 123.68\n",
    "        im = im[:, :, ::-1]  # change to BGR\n",
    "        return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting baseline features from each class videos\n",
    "baseline_features=[[[0 for x in range(baseline_feature_size)] for y in range(nb_videos)] for z in range(nb_activities)]\n",
    "#labels=np.zeros([nb_activities*nb_videos])\n",
    "\n",
    "#ptr=0\n",
    "for nb_act in range(nb_activities):\n",
    "    for nb_vid in range(nb_videos):\n",
    "        temp=0\n",
    "        temp=get_compress_frames_data(dataset_dir+str(activity_folders[nb_act])+'/'+str(activity_subfolders[nb_act][nb_vid]))\n",
    "        print(dataset_dir+str(activity_folders[nb_act])+'/'+str(activity_subfolders[nb_act][nb_vid]))\n",
    "        baseline_features[nb_act][nb_vid]=extract_baseline_video_features(temp)\n",
    "        #labels[ptr]=nb_act\n",
    "        #ptr=ptr+1\n",
    "baseline_features=np.array(baseline_features)\n",
    "#ptr=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_obj = PCA(n_components=2, random_state=0)\n",
    "print(baseline_features.shape)\n",
    "vis_baseline=[pca_obj.fit_transform(baseline_features[i]) for i  in range(nb_activities)]\n",
    "vis_baseline=np.array(vis_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_obj = TSNE(n_components=2, verbose=1)\n",
    "vis_baseline_tsne=[tsne_obj.fit_transform(baseline_features[i]) for i  in range(nb_activities)]\n",
    "vis_baseline_tsne=np.array(vis_baseline_tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hog feature demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data, exposure\n",
    "\n",
    "hog_dataset='/nobackup/leopauly/leeds_reordered/reach/0/'\n",
    "filenames= sorted(os.listdir(hog_dataset))\n",
    "print(filenames)\n",
    "    \n",
    "for file in filenames:\n",
    "        \n",
    "        image = io.imread(hog_dataset+file,-1)\n",
    "        fd, hog_image = hog(image, orientations=nb_orientations, pixels_per_cell=nb_pixels_per_cell,cells_per_block=nb_cells_per_block, visualise=True)\n",
    "        \n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "\n",
    "        ax1.axis('off')\n",
    "        ax1.imshow(image, cmap=plt.cm.gray)\n",
    "        ax1.set_title('Input image')\n",
    "\n",
    "        # Rescale histogram for better display\n",
    "        hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "\n",
    "        ax2.axis('off')\n",
    "        ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
    "        ax2.set_title('Histogram of Oriented Gradients')\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "print(fd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Extraction of features - baseline2\n",
    "def extract_baseline2_video_features(vid):\n",
    "\n",
    "    sum_val=0\n",
    "    for i in range(cluster_length):\n",
    "        \n",
    "        frame_=vid[i]\n",
    "        frame=rgb2grey(frame_)\n",
    "        temp_val,_= hog(frame, orientations=nb_orientations, pixels_per_cell=nb_pixels_per_cell,cells_per_block=nb_cells_per_block, visualise=True) #hog.extract(frame)\n",
    "        temp_val=temp_val.reshape(-1) #print('temp_val',temp_val.shape)\n",
    "        sum_val=sum_val+temp_val  #print('sum_val',sum_val.shape)\n",
    "        \n",
    "    features=sum_val/cluster_length  #print('feature from one video.shape',features.shape)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting baseline features from each class videos\n",
    "baseline2_features=[[[0 for x in range(baseline2_feature_size)] for y in range(nb_videos)] for z in range(nb_activities)]\n",
    "\n",
    "for nb_act in range(nb_activities):\n",
    "    for nb_vid in range(nb_videos):\n",
    "        temp=0\n",
    "        temp=get_compress_frames_data(dataset_dir+str(activity_folders[nb_act])+'/'+str(activity_subfolders[nb_act][nb_vid]))\n",
    "        print(dataset_dir+str(activity_folders[nb_act])+'/'+str(activity_subfolders[nb_act][nb_vid]))\n",
    "        baseline2_features[nb_act][nb_vid]=extract_baseline2_video_features(temp)\n",
    "\n",
    "baseline2_features=np.array(baseline2_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_obj = PCA(n_components=2, random_state=0)\n",
    "vis_baseline2=[pca_obj.fit_transform(baseline2_features[i]) for i  in range(nb_activities)]\n",
    "vis_baseline2=np.array(vis_baseline2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_obj = TSNE(n_components=2, verbose=1)\n",
    "vis_baseline2_tsne=[tsne_obj.fit_transform(baseline2_features[i]) for i  in range(nb_activities)]\n",
    "vis_baseline2_tsne=np.array(vis_baseline2_tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting and visualisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D visualisation of features\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "\n",
    "plt.scatter(vis_activitynet[:,:, 0], vis_activitynet[:,:, 1], c=labels, marker='x',s=300, cmap=plt.cm.brg, alpha=300)\n",
    "#plt.scatter(vis_baseline[:,:, 0], vis_baseline[:,:, 1], c=labels, marker='*', cmap=plt.cm.nipy_spectral, alpha=100)\n",
    "#plt.scatter(vis_baseline2[:,:, 0], vis_baseline2[:,:, 1], c=labels, marker='o', cmap=plt.cm.nipy_spectral, alpha=100)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.scatter(vis_baseline[:,:, 0], vis_baseline[:,:, 1], c=labels, marker='x',s=300, cmap=plt.cm.brg, alpha=100)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.scatter(vis_baseline2[:,:, 0], vis_baseline2[:,:, 1], c=labels, marker='x',s=300, cmap=plt.cm.brg, alpha=100)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TSne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.scatter(vis_activitynet_tsne[:,:, 0], vis_activitynet_tsne[:,:, 1], c=labels, marker='x',s=300, cmap=plt.cm.brg, alpha=100)\n",
    "#plt.scatter(vis_baseline_tsne[:,:, 0], vis_baseline_tsne[:,:, 1], c=labels, marker='*',s=300, cmap=plt.cm.brg, alpha=100)\n",
    "#plt.scatter(vis_baseline2_tsne[:,:, 0], vis_baseline2_tsne[:,:, 1], c=labels, marker='o',s=300, cmap=plt.cm.nipy_spectral, alpha=100)\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "#plt.scatter(vis_activitynet_tsne[:,:, 0], vis_activitynet_tsne[:,:, 1], c=labels, marker='x',s=300, cmap=plt.cm.brg, alpha=100)\n",
    "plt.scatter(vis_baseline_tsne[:,:, 0], vis_baseline_tsne[:,:, 1], c=labels, marker='x',s=300, cmap=plt.cm.brg, alpha=100)\n",
    "#plt.scatter(vis_baseline2_tsne[:,:, 0], vis_baseline2_tsne[:,:, 1], c=labels, marker='x',s=300, cmap=plt.cm.nipy_spectral, alpha=100)\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "#plt.scatter(vis_activitynet_tsne[:,:, 0], vis_activitynet_tsne[:,:, 1], c=labels, marker='x',s=300, cmap=plt.cm.nipy_spectral, alpha=100)\n",
    "#plt.scatter(vis_baseline_tsne[:,:, 0], vis_baseline_tsne[:,:, 1], c=labels, marker='*',s=300, cmap=plt.cm.brg, alpha=100)\n",
    "plt.scatter(vis_baseline2_tsne[:,:, 0], vis_baseline2_tsne[:,:, 1], c=labels, marker='x',s=300, cmap=plt.cm.brg, alpha=100)\n",
    "print(vis_baseline2_tsne.shape)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
