{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp3_Senariowise_CompvsIncomp_Push_V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author : @leopauly | cnlp@leeds.ac.uk <br>\n",
    "Program: Tying to show that the proposed activity feature method can distinguish between complete vs Incomplete actions scenario by scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import PIL.Image as Image\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import skimage\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from skimage.color import grey2rgb,rgb2grey\n",
    "from skimage.feature import hog\n",
    "from skimage import io\n",
    "\n",
    "# Custom scripts\n",
    "import lscript as lsp\n",
    "import modelling as md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "height=112 \n",
    "width=112 \n",
    "channel=3\n",
    "crop_size=112\n",
    "cluster_length=16\n",
    "feature_size=8192 #4096 #16384\n",
    "baseline_feature_size=4608\n",
    "baseline2_feature_size=2\n",
    "\n",
    "\n",
    "nb_activities=4\n",
    "nb_videos=5\n",
    "nb_pixels_per_cell=(8,8)\n",
    "nb_cells_per_block=(1,1)\n",
    "nb_orientations=16\n",
    "saved_path='/nobackup/leopauly/'\n",
    "dataset_dir='/nobackup/leopauly/Activity_Analysis_new/Frames/Pushing/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acitivity net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Defining placeholders in tf for images and targets\n",
    "x_image = tf.placeholder(tf.float32, [None, 16,height,width,channel],name='x') \n",
    "\n",
    "model_keras = md.C3D_ucf101_training_model_tf(summary=False)\n",
    "out=model_keras(x_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting a session - Activity Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Start the session with logging placement.\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False))\n",
    "\n",
    "### Restore model weights from previously saved model\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, os.path.join(saved_path,'activity_model.ckpt-67'))\n",
    "print(\"Model restored from file: %s\" % saved_path,flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Extraction of features - Acitvity net\n",
    "def extract_activitynet_video_features(vid):\n",
    "    print('vid.shape',vid.shape)\n",
    "    vid_=vid.reshape(-1,cluster_length,height,width,channel)\n",
    "    f_v = sess.graph.get_tensor_by_name('flatten_1/Reshape:0')\n",
    "    f_v_val=sess.run([f_v], feed_dict={'conv1_input:0':vid_,x_image:vid_,K.learning_phase(): 0 })\n",
    "    features=np.reshape(f_v_val,(-1))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_compress_frames_data(vid_folder, num_frames_per_clip=cluster_length):\n",
    "  ''' Given a foler with frames from a video, fuction outputs a  num_frames_per_clip images which is the downsampled video'''\n",
    "\n",
    "  ret_arr = []\n",
    "  filenames=os.listdir(vid_folder)\n",
    "  filenames = sorted(filenames)\n",
    "  jump=math.floor((len(filenames)/num_frames_per_clip))\n",
    "  loop=0\n",
    "  for i in range(0,len(filenames),jump):\n",
    "      if (loop>15):\n",
    "        break\n",
    "      if (filenames[i].endswith('.png')):\n",
    "        image_name = str(vid_folder) + '/' + str(filenames[i])\n",
    "        img = skimage.io.imread(image_name)  #Image.open\n",
    "        img=np.array(img)\n",
    "        img = cv2.resize(img,(crop_size,crop_size))\n",
    "        img_data = np.array(img)\n",
    "        ret_arr.append(img_data)\n",
    "        loop=loop+1\n",
    "  ret_arr=np.array(ret_arr)\n",
    "  print('ret_arr',ret_arr.shape)\n",
    "  return np.array(ret_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Extracting activity features from demo video\n",
    "def activity_feature_extractor(folder):\n",
    "    vid_folder=dataset_dir+folder\n",
    "    print('Is the folder present:',os.path.isdir(vid_folder))\n",
    "    temp_vid=get_compress_frames_data(vid_folder)\n",
    "    print('Obtained video shape',temp_vid.shape)\n",
    "    lsp.single_video_inline(temp_vid,0,cluster_length)\n",
    "    activitynet_features=extract_activitynet_video_features(temp_vid)\n",
    "    activitynet_features=np.array(activitynet_features)\n",
    "    return activitynet_featuress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the folder present: True\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_compress_frames_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-86026483cdc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Demo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mactivitynet_features_demo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivity_feature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'V1_colour'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Demo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-b822cbf3c57b>\u001b[0m in \u001b[0;36mactivity_feature_extractor\u001b[0;34m(folder)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvid_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Is the folder present:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtemp_vid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_compress_frames_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Obtained video shape'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemp_vid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlsp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_video_inline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_vid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcluster_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_compress_frames_data' is not defined"
     ]
    }
   ],
   "source": [
    "## Demo\n",
    "activitynet_features_demo=(activity_feature_extractor('V1_colour')) # Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Same speed\n",
    "activitynet_features=(activity_feature_extractor('V1')) \n",
    "distance=np.linalg.norm(activitynet_features_demo-activitynet_features)\n",
    "print(distance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
